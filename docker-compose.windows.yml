services:

  speaches:
    image: ghcr.io/speaches-ai/speaches:latest-cpu
    container_name: speaches
    profiles: [ "tts" ]
    ports:
      - "8969:8000"
    volumes:
      - hf-hub-cache:/home/ubuntu/.cache/huggingface/hub
    restart: unless-stopped

  app:
    build: .
    ports:
      - "5011:5011"
    restart: always
    depends_on:
      db:
        condition: service_healthy
    environment:
      - DATABASE_URL=postgresql://postgres:postgres@db:5432/personal_guru
      - PORT=5011
      # Use env var if set, otherwise fallback to host.docker.internal
      - LLM_BASE_URL=${LLM_BASE_URL:-http://host.docker.internal:11434/v1}
      - LLM_MODEL_NAME=${LLM_MODEL_NAME:-llama3}
      - LLM_API_KEY=${LLM_API_KEY:-}
      - LLM_MAX_OUTPUT_TOKENS=${LLM_MAX_OUTPUT_TOKENS:-20000}
      - TTS_BASE_URL=${DOCKER_TTS_BASE_URL:-http://speaches:8000/v1}
      - STT_BASE_URL=${DOCKER_STT_BASE_URL:-http://speaches:8000/v1}
      - OPENAI_API_KEY=${OPENAI_API_KEY:-}
      - YOUTUBE_API_KEY=${YOUTUBE_API_KEY:-}
      # Force external providers in Docker to avoid dependency issues (e.g. native whisper)
      # docker image lacks 'faster-whisper', so we must use 'externalapi' to use Speaches/OpenAI
      - TTS_PROVIDER=externalapi
      - STT_PROVIDER=externalapi
      # Optimize sandbox performance by using internal container path (not mounted from Windows)
      - SANDBOX_PATH=/app_sandbox
    extra_hosts:
      - "host.docker.internal:host-gateway"
    volumes:
      - .:/app
    command: python run.py

  db:
    image: postgres:15
    restart: always
    environment:
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=postgres
      - POSTGRES_DB=personal_guru
    volumes:
      - ./postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: [ "CMD-SHELL", "pg_isready -U postgres" ]
      interval: 5s
      timeout: 5s
      retries: 5
    ports:
      - "5433:5432"

volumes:
  hf-hub-cache:
